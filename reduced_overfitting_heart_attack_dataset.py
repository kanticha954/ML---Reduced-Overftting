# -*- coding: utf-8 -*-
"""Reduced Overfitting - heart attack dataset

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MV90_QLF_vxKB49JZbJmPe3I-NGv51W0
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt 
import seaborn as sns 
import plotly as py
import plotly.graph_objs as go
from sklearn.cluster import KMeans
from sklearn.model_selection import KFold
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import mean_absolute_error
from sklearn.cluster import AgglomerativeClustering 
import warnings
import os
warnings.filterwarnings("ignore")

from google.colab import drive
drive.mount('/content/drive')

#https://www.kaggle.com/datasets/rashikrahmanpritom/heart-attack-analysis-prediction-dataset/code?datasetId=1226038&searchQuery=hold+out
df = pd.read_csv('/content/drive/MyDrive/ML - Report/heart.csv')
df.head()

df.info()

import scipy.stats as stats
data = np.array(df)
zscores = stats.zscore(data)

print(zscores)

df.describe()

"""ค่า Mean เมื่อเทียบกับค่า Max ในส่วนของ chol หรือ oldpeak นั้นมีค่าที่ต่างกันค่อนข้างมาก ซึ่งแสดงให้เห็นถึงความอ่อนไหวของข้อมูลต่อค่าปกติ

รวมไปถึงเมื่อลองนำค่า Numeric ในชุดข้อมูลมา plot กราฟแล้ว จะเห็นว่าในชุดข้อมูลมีข้อมูลบางตัวที่กระจายตัวโดดออกมา หรือก็คือแสดงให้เห็นว่าชุดข้อมูลนี้มีค่าที่เป็น Outlier อยู่ด้วย
"""

#Outlier Detection
numeric_list = ["age", "trtbps","chol","thalachh","oldpeak","output"]

df_numeric = df.loc[:, numeric_list]
sns.pairplot(df_numeric, hue = "output", diag_kind = "kde")
plt.show()

features = []
for column in df.columns:
    if column != 'output':
        features.append(column)
X = df[features]
y = df['output']

#First Model
kf = KFold(n_splits=4)
mae_train = []
mae_test = []
for train_index, test_index in kf.split(X):
    
   X_train, X_test = X.iloc[train_index], X.iloc[test_index]
   y_train, y_test = y[train_index], y[test_index]
   model = KNeighborsClassifier(n_neighbors=2)
   model.fit(X_train, y_train)
   
   y_train_pred = model.predict(X_train)
   y_test_pred = model.predict(X_test)
   mae_train.append(mean_absolute_error(y_train, y_train_pred))
   mae_test.append(mean_absolute_error(y_test, y_test_pred))

   print("First Model - Train mae: {} Test mae: {}".format(mae_train, mae_test))

from sklearn.metrics import accuracy_score

y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

print("{0} / {1} correct".format(np.sum(y_test == y_pred), len(y_test)))

model.fit(X_train, y_train ) 
answer = model.predict(X_test)
from sklearn.metrics import classification_report
print(classification_report(y_test, answer))

"""จากกราฟดังกล่าว จะเป็นการเปรียบเทียบระหว่าง Error ของ training set กับ test set ซึ่งจะเห็นว่า training error นั้นมีค่าที่น้อยมาก (จากกราฟจะมี MAE หรือ Mean absolute error) นั้นมีค่าต่ำกว่า 0.2 เสียอีก ในขณะที่ testing error นั้นมีค่ามากกว่าอย่างเห็นได้ชัด โดยระยะห่างจะอยู่ที่ระหว่าง 0.3 - 0.8

หรืออาจจะกล่าวได้ว่าในขณะที่นำโมเดลนี้ไปเทรนนั้นอาจจะดูเหมือนได้ผลการทำนายที่แม่นยำ ทว่าเมื่อนำไปปรับใช้กับข้อมูลชุดใหม่ (testing set) ค่าความผิดพลาดนั้นกลับมากกว่าอย่างเห็นได้ชัด ซึ่งสิ่งนี้เองที่แสดงให้เห็นถึงภาวะ Overfitting ของชุดข้อมูลนี้
"""

folds = range(1, kf.get_n_splits() + 1)
plt.plot(folds, mae_train, 'o-', color='green', label='train')
plt.plot(folds, mae_test, 'o-', color='red', label='test')
plt.legend()
plt.grid()
plt.xlabel('Number of fold')
plt.ylabel('Mean Absolute Error')
plt.show()

"""#ลด Overfitting

#1. ลดความซับซ้อนของโมเดล 
ด้วย function MinMaxScaler ซึ่งหมายถึงการ scale ให้ข้อมูลมีค่าระหว่างสูงสุดและต่ำสุดเท่านั้น งเป็นการทำเพื่อที่ข้อมูลจะไม่ได้รับอิทธิพลจากข้อมูลจำพวก Outlier หรือมีค่าผิดปกติ
"""

from sklearn.preprocessing import MinMaxScaler
import numpy as np
for column in X.columns:
    feature = np.array(X[column]).reshape(-1,1)
    scaler = MinMaxScaler()
    scaler.fit(feature)
    feature_scaled = scaler.transform(feature)
    X[column] = feature_scaled.reshape(1,-1)[0]

X.head()

"""จะเห็นว่าหลังทำ MinMaxScaler แล้ว ข้อมูลจะถูกปรับค่าลงมาให้อยู่ในรูปของทศนิยมระหว่าง 0 - 1 ไม่เกินนี้ หลังจากที่ก่อนหน้านี้มีค่าบางอย่างที่เป็นหลักร้อยหรือหลักสิบก็มี"""

kf = KFold(n_splits=4)
mae_train = []
mae_test = []
for train_index, test_index in kf.split(X):
    
   X_train, X_test = X.iloc[train_index], X.iloc[test_index]
   y_train, y_test = y[train_index], y[test_index]

   model_af = KNeighborsClassifier(n_neighbors=2)
   model_af.fit(X_train, y_train)
   y_train_pred = model_af.predict(X_train)
   y_test_pred = model_af.predict(X_test)
   mae_train.append(mean_absolute_error(y_train, y_train_pred))
   mae_test.append(mean_absolute_error(y_test, y_test_pred))

   print("1 Solution - Train mae: {} Test mae: {}".format(mae_train, mae_test))

model_af.fit(X_train, y_train ) 
answer = model_af.predict(X_test)
from sklearn.metrics import classification_report
print(classification_report(y_test, answer))

y_pred = model_af.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

print("{0} / {1} correct".format(np.sum(y_test == y_pred), len(y_test)))

folds = range(1, kf.get_n_splits() + 1)
plt.plot(folds, mae_train, 'o-', color='green', label='train')
plt.plot(folds, mae_test, 'o-', color='red', label='test')
plt.legend()
plt.grid()
plt.xlabel('Number of fold')
plt.ylabel('Mean Absolute Error')
plt.show()

"""ผลลัพธ์ที่ได้คือกราฟดังกล่าวซึ่งค่า Error จาก training set มีแนวโน้มที่จะลดลงมาอย่างเห็นได้ชัด เป็นมีแนวโน้มอยู่ที่ 0.25 - 0.50

#2. เลือก Feature
ใช้การเลือก Features บางส่วนที่มีความสำคัญ เพื่อที่ว่า Model จะไม่จำเป็นต้องเรียนรู้จาก Feature มากมายจนเกินพอดี
"""

from sklearn.feature_selection import chi2
from sklearn.feature_selection import SelectKBest

test = SelectKBest(score_func=chi2, k=4)
fit = test.fit(X, y)
fit.scores_
dfscores = pd.DataFrame(fit.scores_)
dfcol = pd.DataFrame(X.columns)

featureScore = pd.concat([dfcol, dfscores], axis = 1)
featureScore.columns = ['Feature', 'Score']

print(featureScore.nlargest(4, 'Score'))

df.head()

#Get New Dataset after Feature Selection
X_new  = test.fit_transform(X,y)
print(X_new)

print('Original number of features:', X.shape)
print('Reduced number of features:', X_new.shape)
#เลือกมา 4 features

kf = KFold(n_splits=4)
mae_train_2 = []
mae_test_2 = []
for train_index, test_index in kf.split(X_new):
    
   X_train, X_test = X_new[train_index], X_new[test_index]
   y_train, y_test = y[train_index], y[test_index]

   model_af2 = KNeighborsClassifier(n_neighbors=2)
   model_af2.fit(X_train, y_train)
   y_train_pred = model_af2.predict(X_train)
   y_test_pred = model_af2.predict(X_test)
   mae_train_2.append(mean_absolute_error(y_train, y_train_pred))
   mae_test_2.append(mean_absolute_error(y_test, y_test_pred))

   print("2 Solution - Train mae: {} Test mae: {}".format(mae_train_2, mae_test_2))

model_af2.fit(X_train, y_train ) 
answer = model_af2.predict(X_test)
from sklearn.metrics import classification_report
print(classification_report(y_test, answer))

y_pred = model_af2.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

print("{0} / {1} correct".format(np.sum(y_test == y_pred), len(y_test)))

folds = range(1, kf.get_n_splits() + 1)
plt.plot(folds, mae_train_2, 'o-', color='green', label='train')
plt.plot(folds, mae_test_2, 'o-', color='red', label='test')
plt.legend()
plt.grid()
plt.xlabel('Number of fold')
plt.ylabel('Mean Absolute Error')
plt.show()

"""ด้วยการใช้ Feature Selection นี้เอง ทำให้เห็นว่าค่า MAE ของ Model ระหว่าง Training set และ Testing set เริ่มมีค่าเข้าใกล้กันมาขึ้นจากเดิม นี่เองที่ทำให้เห็นว่า Overfitting น้อยลง โดยจะมีแนวโน้มอยู่ที่ 0.2 - 0.55

#3. เปลี่ยนโมเดล

เมื่อเกิดปัญหา Overfiting อีกวิธีที่สามารถช่วยในการลดปัญหา Overfit ได้คือการลองเปลี่ยนไปใช้โมเดลอื่น ๆ
"""

#NaiveByes
from sklearn.naive_bayes import GaussianNB

kf = KFold(n_splits=4)
mae_train_gnb = []
mae_test_gnb = []
gnb = GaussianNB()
for train_index, test_index in kf.split(X):
    
   X_train_gnb, X_test_gnb = X.iloc[train_index], X.iloc[test_index]
   y_train_gnb, y_test_gnb = y[train_index], y[test_index]
   gnb.fit(X_train_gnb, y_train_gnb)
   y_train_pred_gnb = gnb.predict(X_train_gnb)
   y_test_pred_gnb = gnb.predict(X_test_gnb)

   print("Gnb model - Train mae: {} Test mae: {}".format(mae_train_gnb, mae_test_gnb))

   mae_train_gnb.append(mean_absolute_error(y_train_gnb, y_train_pred_gnb))
   mae_test_gnb.append(mean_absolute_error(y_test_gnb, y_test_pred_gnb))

gnb.fit(X_train_gnb, y_train_gnb) 
answer = gnb.predict(X_test_gnb)
from sklearn.metrics import classification_report
print(classification_report(y_test_gnb, answer))

y_pred = gnb.predict(X_test_gnb)
accuracy = accuracy_score(y_test_gnb, y_pred)
print("Accuracy:", accuracy)

print("{0} / {1} correct".format(np.sum(y_test_gnb == y_pred), len(y_test_gnb)))

folds = range(1, kf.get_n_splits() + 1)
plt.plot(folds, mae_train_gnb, 'o-', color='green', label='train')
plt.plot(folds, mae_test_gnb, 'o-', color='red', label='test')
plt.legend()
plt.grid()
plt.xlabel('Number of fold')
plt.ylabel('Mean Absolute Error')
plt.show()

"""โดยตัวโมเดลแรกที่เลือกมาทำคือ Naive Bayes เนื่องจากเป็นโมเดลที่ไม่ได้มีความซับซ้อนมากและค่อนข้างเรียบง่ายจากการที่มันเป็น  Simple (linear) hypothesis function ทำให้ลดปัญหาการเกิด Overfit ได้
ซึ่งจะเห็นว่าค่า MAE ที่ได้มานั้นมีค่าอยู่ที่ระหว่าง 0.20 - 0.35 เท่านั้น
"""

#LogisticRegression
from collections import OrderedDict
from sklearn.linear_model import LogisticRegression

kf = KFold(n_splits=4)
lg = LogisticRegression()
mae_train_lg = []
mae_test_lg = []
train_mae, test_mae = [], []
for train_index, test_index in kf.split(X):

  X_train, X_test = X.iloc[train_index], X.iloc[test_index]
  y_train, y_test = y[train_index], y[test_index]

  lg.fit(X_train, y_train)
  y_train_predicted = lg.predict(X_train)
  y_test_predicted = lg.predict(X_test)

  mae_train_lg = mean_absolute_error(y_train,  y_train_predicted)
  mae_test_lg = mean_absolute_error(y_test, y_test_predicted)

  train_mae += [mae_train_lg]
  test_mae += [mae_test_lg]

  print("LG model - Train mae : {} Test mae: {}".format(mae_train_lg, mae_test_lg))

lg.fit(X_train, y_train ) 
answer = lg.predict(X_test)
from sklearn.metrics import classification_report
print(classification_report(y_test, answer))

print(accuracy_score(y_test,y_test_predicted))

print("{0} / {1} correct".format(np.sum(y_test == y_test_predicted), len(y_test)))

folds = range(1, kf.get_n_splits() + 1)
plt.plot(folds, train_mae, 'o-', color='green', label='train')
plt.plot(folds, test_mae , 'o-', color='red', label='test')
plt.legend()
plt.grid()
plt.xlabel('Number of fold')
plt.ylabel('Mean Absolute Error')
plt.show()

"""ถัดมาอีกโมเดลที่ได้นำมาทดลองใช้กับ Dataset นี้คือ LinearRegressor โดยผลลัพธ์ที่ได้คือค่า MAE ได้ลดมาอยู่ในช่วง 0.26 - 0.44 โดยประมาณ"""